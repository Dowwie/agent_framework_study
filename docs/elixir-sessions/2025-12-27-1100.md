# Elixir Design Session - 2025-12-27 (Session 2)

## Session Context
- **Prior sessions**:
  - `2025-12-24-0639.md` (Dimensions 1-13)
  - `2025-12-25-0409.md` (Dimensions 14-17, gap resolution)
  - `2025-12-27-0639.md` (Dimensions 20-21, critique evaluation)
- **Consolidated design**: `docs/elixir-design.md` (21 dimensions documented)
- **Research backlog**: `docs/TODO-ANALYSIS.md` (5 research items)
- **Analysis artifacts**:
  - `reports/synthesis/comparison-matrix.md`
  - `reports/synthesis/antipatterns.md`
  - `reports/synthesis/reference-architecture.md`
- **Session goal**: Continue design refinement or implementation planning

---

## Session Resume Summary

### Completed Dimensions (21)

| # | Dimension | Decision |
|---|-----------|----------|
| 1 | Core Architecture | Deep Agents via GenServer + Task |
| 2 | Process Structure | SessionController GenServer + Task.async for steps |
| 3 | State Model | PostgreSQL + Redis with Ecto embedded schemas |
| 4 | Message Protocol | Lightweight reference structs (session_id, plan_id, step_index) |
| 5 | ETS Layout | 5 tables: :sessions, :messages, :context, :cache, :sub_agent_contexts |
| 6 | Clustering | Node affinity + ex_hash_ring + write-behind persistence |
| 7 | Tool System | Ecto schemas, introspection, ToolError, sandbox_mode callback |
| 8 | LLM Integration | Req + Behaviour adapters, PubSub streaming, Hammer, :fuse |
| 9 | Memory/Context | Token budget + eviction, pgvector + pg_bm25, tiktoken NIF |
| 10 | Multi-Agent | SessionController orchestrates sub-agents; hub-and-spoke |
| 11 | Observability | Telemetry + OpenTelemetry, step-level granularity |
| 12 | Error Handling | Feed errors to LLM, checkpoint recovery, max iterations |
| 13 | Interrupts/Breakpoints | Hybrid (step-level + runtime registry), timeout with default |
| 14 | Human-in-the-Loop | Step type + tool, PubSub → Phoenix Channel → WebSocket |
| 15 | Time Travel Debugging | Hybrid snapshots + events, L1 view-only inspection |
| 16 | Sub-Agent Architecture | Task-based with ETS context; ephemeral processes, persistent context |
| 17 | Artifact/Workspace Memory | S3-compatible storage with behaviour abstraction |
| 18 | System Prompts | EEx templates in priv/prompts/; directory override |
| 19 | Context Handoff | Hybrid extraction: code facts + LLM interpretation |
| 20 | Hierarchical Memory | 50/30/20 split: recent verbatim, historical summary, semantic retrieval |
| 21 | Sandbox Interface | Behaviour + WebSocket protocol for external code execution sandbox |

### Critique Items Resolved

| Recommendation | Status | Notes |
|----------------|--------|-------|
| BSP Supersteps | Skipped | GenServer provides ordering; solves non-problem |
| 50/30/20 Memory | Adopted | Dimension 20 |
| Tool Sandboxing | Adopted (interface) | Dimension 21 |
| Error-as-Data | Adopted | ToolError schema refinement |
| Tagged IDs | Skipped | Marginal benefit; Ecto validates at boundaries |
| Agno Taxonomy | Deferred | Research task in TODO-ANALYSIS.md |

### Research Items Pending

1. Tiktoken Elixir bindings (Medium priority - needed for memory strategy)
2. Context compaction alternatives (Medium priority)
3. LiteLLM provider patterns (Low priority)
4. Agno event taxonomy (Low priority)
5. OpenTelemetry GenAI conventions (Low priority)

---

## Gap Analysis

### Dimension 20: Hierarchical Memory - Gaps

| Documented | Missing |
|------------|---------|
| 50/30/20 split concept | When is summary generation triggered? (threshold?) |
| ContextBuilder sketch | Summary cache TTL and invalidation |
| "Background Task" mention | How semantic retrieval queries pgvector |
| | What assemble_context outputs (format) |
| | Integration point: when is build() called in the agent loop? |

### Dimension 21: Sandbox Interface - Gaps

| Documented | Missing |
|------------|---------|
| Behaviour definition | Connection lifecycle (reconnect, health checks) |
| WebSocket protocol (happy path) | Protocol error cases (disconnect mid-execution, malformed response) |
| sandbox_mode/0 callback | How tool executor routes based on mode |
| | How streaming events are collected into result |
| | Configuration (sandbox URL, auth, connection pool?) |

### ToolError - Gaps

| Documented | Missing |
|------------|---------|
| Schema definition | Integration with tool executor flow |
| Constructor helpers | Where format_for_llm lives and when it's called |
| LLM feedback format example | How existing {:error, reason} returns migrate |

---

## Session Continuation

### Dimension 20, Gap 1: Summary Generation Trigger

**Decision:** Token threshold trigger with full replacement

**Specification:**
```elixir
@summary_token_threshold 4_000  # Unsummarized tokens before regeneration
@summary_target_tokens 2_000    # Target size for generated summary

defp maybe_regenerate_summary(session_id) do
  unsummarized_tokens = count_unsummarized_tokens(session_id)

  if unsummarized_tokens > @summary_token_threshold do
    # Spawn background task, don't block
    Task.Supervisor.start_child(SummaryTaskSupervisor, fn ->
      regenerate_summary(session_id)
    end)
  end
end

defp regenerate_summary(session_id) do
  # Load all messages not in current summary
  messages = load_unsummarized_messages(session_id)

  # Full replacement: generate new summary from scratch
  {:ok, summary} = LLM.summarize(messages, target_tokens: @summary_target_tokens)

  # Cache in ETS, persist to Postgres
  cache_summary(session_id, summary)
  persist_summary(session_id, summary)
end
```

**Trigger points:**
- After each LLM response is stored (async check)
- On session resume from checkpoint

**Rationale:**
- Token threshold aligns with memory strategy (we're managing token budgets)
- Full replacement avoids drift; summary accurately reflects source
- Background task prevents blocking the agent loop

---

### Dimension 20, Gap 2: Summary Cache TTL and Invalidation

**Decision:** No TTL, token-triggered only

**Specification:**
```elixir
defmodule AgentFramework.Memory.SummaryCache do
  @ets_table :context

  def get(session_id) do
    case :ets.lookup(@ets_table, {:summary, session_id}) do
      [{_, summary, _metadata}] -> {:ok, summary}
      [] -> :miss
    end
  end

  def put(session_id, summary) do
    metadata = %{
      generated_at: DateTime.utc_now(),
      messages_covered_through: last_message_id,
      token_count: count_tokens(summary)
    }
    :ets.insert(@ets_table, {{:summary, session_id}, summary, metadata})
  end

  # No explicit invalidation needed; regeneration replaces
end
```

**Cache behavior:**
- Summary remains in ETS until regenerated
- Regeneration (from Gap 1) does atomic replacement via `:ets.insert`
- On session recovery: load from Postgres if ETS miss
- On session end: summary persisted to Postgres, ETS entry cleaned up

**Rationale:**
- Token threshold trigger already handles freshness
- No TTL complexity; summary is valid until replaced
- Metadata tracks coverage for debugging/observability

---

### Dimension 20, Gap 3: Semantic Retrieval via pgvector

**Decision:** Per-message embedding, query by current user message, top-K with token accumulation

**Embedding strategy:**
- Each message stored with its embedding vector
- Embeddings generated on message creation (async, non-blocking)
- Stored in Postgres with pgvector extension

**Schema addition:**
```elixir
# In Postgres (Ecto migration)
create table(:message_embeddings) do
  add :message_id, references(:messages, type: :binary_id, on_delete: :delete_all)
  add :session_id, :binary_id, null: false
  add :embedding, :vector, size: 1536  # OpenAI ada-002 dimension
  add :token_count, :integer

  timestamps()
end

create index(:message_embeddings, [:session_id])
create index(:message_embeddings, ["embedding vector_cosine_ops"], using: :ivfflat)
```

**Retrieval implementation:**
```elixir
defmodule AgentFramework.Memory.SemanticSearch do
  @top_k 20  # Fetch more than needed, filter by budget

  def retrieve(session_id, query_embedding, opts) do
    budget = Keyword.fetch!(opts, :budget)
    exclude_ids = Keyword.get(opts, :exclude_message_ids, [])

    # Query pgvector for top-K similar messages
    messages =
      from(e in MessageEmbedding,
        where: e.session_id == ^session_id,
        where: e.message_id not in ^exclude_ids,
        order_by: fragment("embedding <=> ?", ^query_embedding),
        limit: @top_k,
        preload: [:message]
      )
      |> Repo.all()

    # Accumulate until budget exhausted
    accumulate_to_budget(messages, budget)
  end

  defp accumulate_to_budget(messages, budget) do
    messages
    |> Enum.reduce_while({[], 0}, fn msg, {acc, tokens} ->
      new_tokens = tokens + msg.token_count
      if new_tokens <= budget do
        {:cont, {[msg.message | acc], new_tokens}}
      else
        {:halt, {acc, tokens}}
      end
    end)
    |> elem(0)
    |> Enum.reverse()
  end
end
```

**Query embedding source:**
- Generated from current user message content
- Passed into `ContextBuilder.build/3` as `query_embedding` option
- If no user message (e.g., continuation), use step description

**Rationale:**
- Per-message is simple and sufficient for conversation context
- User message query aligns with user intent
- Top-K with accumulation respects token budget deterministically
- Excluding recent messages (already in verbatim tier) avoids duplication

---

### Dimension 20, Gap 4: assemble_context Output Format

**Decision:** Structured Context struct with semantic attribution

**Context schema:**
```elixir
defmodule AgentFramework.Schemas.Context do
  @moduledoc """
  Assembled context ready for LLM consumption.
  Preserves tier provenance for debugging and observability.
  """

  use Ecto.Schema

  @type t :: %__MODULE__{
    session_id: String.t(),
    summary: String.t() | nil,
    recent_messages: [Message.t()],
    semantic_messages: [Message.t()],
    total_tokens: non_neg_integer(),
    metadata: map()
  }

  @primary_key false
  embedded_schema do
    field :session_id, :binary_id
    field :summary, :string
    embeds_many :recent_messages, Message
    embeds_many :semantic_messages, Message
    field :total_tokens, :integer
    field :metadata, :map, default: %{}
  end
end
```

**Assembly implementation:**
```elixir
defmodule AgentFramework.Memory.ContextBuilder do
  @semantic_attribution "[From earlier in conversation]"

  defp assemble_context(tiers) do
    [recent, summary, semantic] = tiers

    %Context{
      session_id: extract_session_id(recent),
      summary: summary,
      recent_messages: recent,
      semantic_messages: attribute_semantic(semantic),
      total_tokens: count_total_tokens(tiers),
      metadata: %{
        recent_count: length(recent),
        semantic_count: length(semantic),
        has_summary: summary != nil,
        built_at: DateTime.utc_now()
      }
    }
  end

  defp attribute_semantic(messages) do
    Enum.map(messages, fn msg ->
      %{msg | content: "#{@semantic_attribution}\n#{msg.content}"}
    end)
  end
end
```

**LLM client formatting (in LLM module):**
```elixir
def format_context_for_llm(%Context{} = ctx, system_prompt) do
  # Build messages list for API
  messages = []

  # System prompt first
  messages = messages ++ [%{role: "system", content: system_prompt}]

  # Summary as system context if present
  if ctx.summary do
    messages = messages ++ [%{role: "system", content: "[Conversation Summary]\n#{ctx.summary}"}]
  end

  # Semantic messages (attributed, chronological within tier)
  messages = messages ++ format_messages(ctx.semantic_messages)

  # Recent messages (verbatim, chronological)
  messages = messages ++ format_messages(ctx.recent_messages)

  messages
end
```

**Rationale:**
- Struct preserves tier boundaries for observability (can log what came from where)
- Attribution helps LLM understand temporal context
- LLM client owns final formatting (different providers may have different requirements)
- Metadata useful for debugging and telemetry

---

### Dimension 20, Gap 5: Integration Point

**Decision:** Build context once per LLM call, in StepExecutor

**Call site in StepExecutor:**
```elixir
defmodule AgentFramework.Session.StepExecutor do
  @token_budget 8_000  # Configurable per model

  def execute_llm_step(session_id, step, opts) do
    # Get query embedding from user message or step description
    query_embedding = get_query_embedding(session_id, step)

    # Build fresh context for this LLM call
    context = ContextBuilder.build(session_id, @token_budget,
      query_embedding: query_embedding
    )

    # Get system prompt for this step type
    system_prompt = Prompts.get(step.type, step: step)

    # Format for LLM and make call
    messages = LLM.format_context_for_llm(context, system_prompt)

    case LLM.chat(messages, opts) do
      {:ok, response} ->
        # Store response, may trigger summary regeneration (async)
        store_response(session_id, response)
        handle_response(session_id, step, response)

      {:error, reason} ->
        {:error, reason}
    end
  end

  # Tool loop: each iteration rebuilds context
  defp handle_response(session_id, step, %{tool_calls: calls}) when calls != [] do
    results = execute_tools(calls)
    store_tool_results(session_id, results)

    # Recursive call rebuilds context with new tool results included
    execute_llm_step(session_id, step, continue: true)
  end

  defp handle_response(_session_id, _step, response) do
    {:ok, response}
  end
end
```

**Summary regeneration trigger:**
```elixir
defp store_response(session_id, response) do
  # Persist response to ETS/Postgres
  Messages.store(session_id, response)

  # Check if summary regeneration needed (async, non-blocking)
  SummaryGenerator.maybe_regenerate(session_id)
end
```

**Rationale:**
- Fresh context per LLM call handles tool loop iterations correctly
- Tool results are stored between calls, included in next context build
- Summary regeneration triggered after storage, runs in background
- Clear data flow: build → format → call → store → (maybe regenerate)

---

### Dimension 20: Complete

All gaps filled for Hierarchical Memory Strategy:

| Gap | Decision |
|-----|----------|
| Trigger | Token threshold (4000 unsummarized) |
| Mode | Full replacement |
| TTL | None; token-triggered only |
| Semantic retrieval | Per-message embedding, user message query, top-K accumulation |
| Output format | Structured Context struct with attribution |
| Integration | Once per LLM call in StepExecutor |

---

## Dimension 21: Sandbox Interface (Gap Resolution)

### Dimension 21, Gap 1: Connection Lifecycle

**Decision:** Pooled connections with immediate + exponential backoff reconnection

**Pool configuration:**
```elixir
# In application.ex supervision tree
children = [
  # ... other children ...
  :poolboy.child_spec(:sandbox_pool,
    name: {:local, :sandbox_pool},
    worker_module: AgentFramework.Sandbox.WebSocketWorker,
    size: 5,           # Base pool size
    max_overflow: 10   # Allow bursts up to 15
  )
]
```

**WebSocket worker with reconnection:**
```elixir
defmodule AgentFramework.Sandbox.WebSocketWorker do
  use GenServer

  @initial_backoff_ms 100
  @max_backoff_ms 30_000
  @max_retries 10

  defstruct [:socket, :url, :auth, :backoff_ms, :retry_count, :pending_executions]

  def init(opts) do
    url = Keyword.fetch!(opts, :url)
    auth = Keyword.get(opts, :auth)

    state = %__MODULE__{
      url: url,
      auth: auth,
      backoff_ms: @initial_backoff_ms,
      retry_count: 0,
      pending_executions: %{}
    }

    {:ok, state, {:continue, :connect}}
  end

  def handle_continue(:connect, state) do
    case connect_websocket(state.url, state.auth) do
      {:ok, socket} ->
        # Reset backoff on successful connection
        {:noreply, %{state | socket: socket, backoff_ms: @initial_backoff_ms, retry_count: 0}}

      {:error, reason} ->
        handle_connection_failure(state, reason)
    end
  end

  defp handle_connection_failure(state, _reason) do
    if state.retry_count >= @max_retries do
      # Fail pending executions, stop worker (supervisor will restart)
      fail_pending(state.pending_executions, :sandbox_unavailable)
      {:stop, :max_retries_exceeded, state}
    else
      # Immediate retry first, then backoff
      delay = if state.retry_count == 0, do: 0, else: state.backoff_ms
      Process.send_after(self(), :reconnect, delay)

      new_backoff = min(state.backoff_ms * 2, @max_backoff_ms)
      {:noreply, %{state | backoff_ms: new_backoff, retry_count: state.retry_count + 1}}
    end
  end

  def handle_info(:reconnect, state) do
    {:noreply, state, {:continue, :connect}}
  end

  # Health check callback for Poolboy
  def handle_call(:health_check, _from, %{socket: nil} = state) do
    {:reply, {:error, :disconnected}, state}
  end
  def handle_call(:health_check, _from, state) do
    {:reply, :ok, state}
  end
end
```

**Pool checkout with health check:**
```elixir
defmodule AgentFramework.Sandbox.Client do
  def execute(request) do
    :poolboy.transaction(:sandbox_pool, fn worker ->
      case GenServer.call(worker, :health_check) do
        :ok ->
          GenServer.call(worker, {:execute, request}, :infinity)
        {:error, reason} ->
          {:error, %ToolError{error_type: :sandbox, message: "Sandbox unavailable: #{reason}"}}
      end
    end)
  end
end
```

**Rationale:**
- Pool amortizes connection overhead across sessions
- Immediate retry handles transient network blips
- Exponential backoff prevents thundering herd on prolonged outage
- Health check before execute prevents wasted time on dead connections
- Supervisor restarts workers that exceed max retries

---

### Dimension 21, Gap 2: Protocol Error Cases

**Decision:** Fail with retryable error on disconnect; log and fail on malformed responses

**Error handling in WebSocketWorker:**
```elixir
defmodule AgentFramework.Sandbox.WebSocketWorker do
  require Logger

  # Handle incoming WebSocket messages
  def handle_info({:websocket, :message, raw_data}, state) do
    case Jason.decode(raw_data) do
      {:ok, message} ->
        handle_protocol_message(message, state)

      {:error, decode_error} ->
        # Log malformed response for debugging
        Logger.error("Sandbox malformed response",
          raw_data: raw_data,
          decode_error: decode_error,
          execution_ids: Map.keys(state.pending_executions)
        )

        # Fail all pending executions (can't determine which one failed)
        fail_pending(state.pending_executions, :malformed_response)
        {:noreply, %{state | pending_executions: %{}}}
    end
  end

  # Handle WebSocket disconnect
  def handle_info({:websocket, :closed, reason}, state) do
    Logger.warning("Sandbox connection closed", reason: reason)

    # Fail pending executions with retryable error
    fail_pending(state.pending_executions, :connection_closed, retryable: true)

    # Trigger reconnection
    {:noreply, %{state | socket: nil, pending_executions: %{}}, {:continue, :connect}}
  end

  # Handle execution timeout
  def handle_info({:execution_timeout, exec_id}, state) do
    case Map.pop(state.pending_executions, exec_id) do
      {nil, _} ->
        # Already completed or failed
        {:noreply, state}

      {caller, new_pending} ->
        # Send timeout error to caller
        error = ToolError.timeout_error("code_execute", state.timeout_ms)
        GenServer.reply(caller, {:error, error})

        # Send cancel to sandbox (best effort)
        send_cancel(state.socket, exec_id)

        {:noreply, %{state | pending_executions: new_pending}}
    end
  end

  defp handle_protocol_message(%{"type" => "error", "id" => id, "message" => msg}, state) do
    case Map.pop(state.pending_executions, id) do
      {nil, _} -> {:noreply, state}
      {caller, new_pending} ->
        error = ToolError.execution_error("code_execute", msg, retryable: false)
        GenServer.reply(caller, {:error, error})
        {:noreply, %{state | pending_executions: new_pending}}
    end
  end

  defp handle_protocol_message(%{"type" => type} = msg, state) when type in ~w(stdout stderr status exit_code) do
    # Accumulate streaming events
    {:noreply, accumulate_event(state, msg)}
  end

  defp fail_pending(pending, reason, opts \\ []) do
    retryable = Keyword.get(opts, :retryable, false)

    Enum.each(pending, fn {_exec_id, caller} ->
      error = ToolError.execution_error("code_execute",
        "Sandbox error: #{reason}",
        retryable: retryable
      )
      GenServer.reply(caller, {:error, error})
    end)
  end
end
```

**Error type mapping:**

| Scenario | ToolError type | Retryable | Notes |
|----------|----------------|-----------|-------|
| Disconnect mid-execution | `:sandbox` | Yes | Connection may restore |
| Malformed response | `:sandbox` | No | Indicates bug, don't retry same request |
| Execution timeout | `:timeout` | No | Code took too long |
| Server-reported error | `:execution` | No | Sandbox explicitly rejected |
| Pool exhausted | `:sandbox` | Yes | Transient capacity issue |

**Rationale:**
- Retryable errors let the agent loop decide whether to retry
- Malformed responses are logged with full context for debugging
- Non-retryable errors prevent infinite retry loops
- Timeout handling includes cancel message to clean up server-side

---

### Dimension 21, Gap 3: Tool Executor Routing

**Decision:** Check sandbox_mode at execution time

**ToolExecutor implementation:**
```elixir
defmodule AgentFramework.Tools.Executor do
  alias AgentFramework.Sandbox.Client, as: SandboxClient
  alias AgentFramework.Schemas.ToolError

  @doc """
  Execute a tool call, routing to sandbox if required.
  """
  def execute(tool_module, params, context) do
    # Validate params via changeset
    case validate_params(tool_module, params) do
      {:ok, validated} ->
        execute_validated(tool_module, validated, context)

      {:error, changeset} ->
        {:error, ToolError.validation_error(
          tool_name(tool_module),
          format_changeset_errors(changeset),
          %{params: params}
        )}
    end
  end

  defp execute_validated(tool_module, params, context) do
    case tool_module.sandbox_mode() do
      :none ->
        # Direct execution in-process
        execute_direct(tool_module, params, context)

      :external ->
        # Route through sandbox
        execute_sandboxed(tool_module, params, context)
    end
  end

  defp execute_direct(tool_module, params, context) do
    # Wrap in Task for isolation (crash doesn't take down caller)
    task = Task.Supervisor.async_nolink(ToolTaskSupervisor, fn ->
      tool_module.execute(params, context)
    end)

    case Task.yield(task, context.timeout_ms) || Task.shutdown(task) do
      {:ok, result} -> result
      nil -> {:error, ToolError.timeout_error(tool_name(tool_module), context.timeout_ms)}
      {:exit, reason} -> {:error, ToolError.execution_error(tool_name(tool_module), inspect(reason))}
    end
  end

  defp execute_sandboxed(tool_module, params, context) do
    # Build sandbox execution request
    request = %{
      id: generate_execution_id(),
      language: params.language,
      code: params.code,
      stdin: Map.get(params, :stdin),
      env: context.sandbox_env || %{},
      limits: %{
        timeout_ms: params.timeout_ms || 30_000,
        memory_mb: context.sandbox_memory_mb || 256,
        cpu_shares: context.sandbox_cpu_shares || 512
      }
    }

    # Execute via sandbox client (pooled connection)
    case SandboxClient.execute(request) do
      {:ok, result} ->
        {:ok, format_sandbox_result(result)}

      {:error, %ToolError{} = error} ->
        {:error, error}
    end
  end

  defp tool_name(module) do
    module
    |> Module.split()
    |> List.last()
    |> Macro.underscore()
  end
end
```

**Rationale:**
- Runtime check is simple and always reflects current tool configuration
- Function call overhead is negligible compared to tool execution time
- No cache invalidation complexity
- Clear routing logic in one place

---

### Dimension 21, Gap 4: Fathom Sandbox Protocol (FSP)

**Decision:** Define a complete application-level protocol for agent-sandbox communication.

## Fathom Sandbox Protocol v1.0

### Overview

| Property | Value |
|----------|-------|
| Encoding | JSON over WebSocket |
| Versioning | Included in every message |
| Output mode | Streaming only |
| Session model | Stateless (fresh environment per execution) |
| Error format | Enumerated codes + message |

### Message Envelope

Every message follows this structure:

```json
{
  "v": 1,
  "type": "<message_type>",
  "id": "<execution_id>",
  "ts": "<ISO8601 timestamp>",
  ...type-specific fields
}
```

| Field | Required | Description |
|-------|----------|-------------|
| `v` | Yes | Protocol version (integer) |
| `type` | Yes | Message type identifier |
| `id` | Conditional | Execution ID (required for execution-related messages) |
| `ts` | Yes | ISO8601 timestamp with milliseconds |

### Client → Sandbox Messages

#### `execute` - Start Code Execution

```json
{
  "v": 1,
  "type": "execute",
  "id": "exec_a1b2c3d4",
  "ts": "2025-12-27T11:30:00.000Z",
  "language": "python",
  "code": "print('hello world')",
  "stdin": null,
  "env": {
    "API_KEY": "..."
  },
  "limits": {
    "timeout_ms": 30000,
    "memory_mb": 256,
    "cpu_shares": 512,
    "max_output_bytes": 1048576
  }
}
```

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `language` | enum | Yes | `python`, `javascript`, `shell`, `elixir` |
| `code` | string | Yes | Source code to execute |
| `stdin` | string | No | Standard input to provide |
| `env` | object | No | Environment variables |
| `limits.timeout_ms` | integer | Yes | Max execution time |
| `limits.memory_mb` | integer | Yes | Max memory allocation |
| `limits.cpu_shares` | integer | No | CPU resource share (default 512) |
| `limits.max_output_bytes` | integer | No | Max stdout+stderr size (default 1MB) |

#### `cancel` - Abort Execution

```json
{
  "v": 1,
  "type": "cancel",
  "id": "exec_a1b2c3d4",
  "ts": "2025-12-27T11:30:05.000Z"
}
```

#### `ping` - Health Check

```json
{
  "v": 1,
  "type": "ping",
  "ts": "2025-12-27T11:30:00.000Z"
}
```

### Sandbox → Client Messages

#### `ack` - Execution Accepted

Sent immediately after `execute` received, before execution begins.

```json
{
  "v": 1,
  "type": "ack",
  "id": "exec_a1b2c3d4",
  "ts": "2025-12-27T11:30:00.001Z"
}
```

#### `status` - Execution State Change

```json
{
  "v": 1,
  "type": "status",
  "id": "exec_a1b2c3d4",
  "ts": "2025-12-27T11:30:00.010Z",
  "status": "running"
}
```

| Status | Meaning |
|--------|---------|
| `running` | Execution started |
| `completed` | Execution finished normally |
| `failed` | Execution encountered error |
| `cancelled` | Execution was cancelled by client |
| `timeout` | Execution exceeded time limit |
| `oom` | Execution exceeded memory limit |

#### `stdout` / `stderr` - Output Streams

```json
{
  "v": 1,
  "type": "stdout",
  "id": "exec_a1b2c3d4",
  "ts": "2025-12-27T11:30:00.100Z",
  "data": "hello world\n"
}
```

- Chunks may arrive in any order relative to `stderr`
- Data is UTF-8 encoded
- Large outputs may be split across multiple messages

#### `result` - Execution Complete

Sent after terminal `status` (completed, failed, timeout, oom, cancelled).

```json
{
  "v": 1,
  "type": "result",
  "id": "exec_a1b2c3d4",
  "ts": "2025-12-27T11:30:01.500Z",
  "exit_code": 0,
  "duration_ms": 1500,
  "resource_usage": {
    "peak_memory_mb": 45,
    "cpu_time_ms": 120
  }
}
```

| Field | Type | Description |
|-------|------|-------------|
| `exit_code` | integer | Process exit code (null if killed) |
| `duration_ms` | integer | Wall-clock execution time |
| `resource_usage` | object | Optional resource metrics |

#### `error` - Execution Error

Sent when execution cannot complete due to sandbox/infrastructure issue.

```json
{
  "v": 1,
  "type": "error",
  "id": "exec_a1b2c3d4",
  "ts": "2025-12-27T11:30:00.500Z",
  "code": "LANGUAGE_NOT_SUPPORTED",
  "message": "Language 'rust' is not available in this sandbox",
  "retryable": false
}
```

### Error Codes

| Code | Category | Retryable | Description |
|------|----------|-----------|-------------|
| `TIMEOUT` | Resource | No | Execution exceeded time limit |
| `OOM` | Resource | No | Execution exceeded memory limit |
| `OUTPUT_LIMIT` | Resource | No | Output exceeded max_output_bytes |
| `LANGUAGE_NOT_SUPPORTED` | Protocol | No | Requested language unavailable |
| `INVALID_REQUEST` | Protocol | No | Malformed request |
| `UNKNOWN_EXECUTION` | Protocol | No | Cancel for unknown execution ID |
| `SANDBOX_OVERLOADED` | Infrastructure | Yes | Sandbox at capacity |
| `INTERNAL_ERROR` | Infrastructure | Yes | Unexpected sandbox failure |
| `NETWORK_ERROR` | Infrastructure | Yes | Sandbox network issue |

#### `pong` - Health Check Response

```json
{
  "v": 1,
  "type": "pong",
  "ts": "2025-12-27T11:30:00.001Z",
  "load": {
    "active_executions": 3,
    "queue_depth": 0
  }
}
```

### Message Sequence Diagrams

**Successful Execution:**
```
Client                    Sandbox
  |                         |
  |-------- execute ------->|
  |<-------- ack -----------|
  |<------- status:running -|
  |<------- stdout ---------|
  |<------- stdout ---------|
  |<------- stderr ---------|
  |<------ status:completed-|
  |<------- result ---------|
  |                         |
```

**Timeout:**
```
Client                    Sandbox
  |                         |
  |-------- execute ------->|
  |<-------- ack -----------|
  |<------- status:running -|
  |<------- stdout ---------|
  |          ... time passes ...
  |<------ status:timeout --|
  |<------- result ---------|
  |                         |
```

**Client Cancel:**
```
Client                    Sandbox
  |                         |
  |-------- execute ------->|
  |<-------- ack -----------|
  |<------- status:running -|
  |-------- cancel -------->|
  |<----- status:cancelled -|
  |<------- result ---------|
  |                         |
```

**Infrastructure Error:**
```
Client                    Sandbox
  |                         |
  |-------- execute ------->|
  |<-------- ack -----------|
  |<------- status:running -|
  |          ... sandbox crashes ...
  |<------- error ----------|
  |         (INTERNAL_ERROR)|
  |                         |
```

### Implementation Notes

**Client-side accumulation:**
```elixir
defmodule AgentFramework.Sandbox.Accumulator do
  defstruct [
    :execution_id,
    :status,
    stdout: "",
    stderr: "",
    exit_code: nil,
    duration_ms: nil,
    resource_usage: nil
  ]

  def new(execution_id) do
    %__MODULE__{execution_id: execution_id}
  end

  def apply_event(acc, %{"type" => "stdout", "data" => data}) do
    %{acc | stdout: acc.stdout <> data}
  end

  def apply_event(acc, %{"type" => "stderr", "data" => data}) do
    %{acc | stderr: acc.stderr <> data}
  end

  def apply_event(acc, %{"type" => "status", "status" => status}) do
    %{acc | status: String.to_existing_atom(status)}
  end

  def apply_event(acc, %{"type" => "result"} = msg) do
    %{acc |
      exit_code: msg["exit_code"],
      duration_ms: msg["duration_ms"],
      resource_usage: msg["resource_usage"]
    }
  end

  def terminal?(%{status: status}) when status in [:completed, :failed, :timeout, :oom, :cancelled] do
    true
  end
  def terminal?(_), do: false

  def to_result(%__MODULE__{} = acc) do
    %{
      execution_id: acc.execution_id,
      status: acc.status,
      exit_code: acc.exit_code,
      stdout: acc.stdout,
      stderr: acc.stderr,
      duration_ms: acc.duration_ms,
      resource_usage: acc.resource_usage
    }
  end
end
```

**Protocol versioning:**
- Clients MUST include `v` in all messages
- Sandbox SHOULD reject messages with unsupported version
- Sandbox MAY support multiple versions simultaneously
- Breaking changes require version bump

---

### Dimension 21, Gap 5: Sandbox Configuration

**Decision:** Runtime configuration via application env; API key auth with refresh token support

**Configuration schema:**
```elixir
# config/runtime.exs
config :agent_framework, AgentFramework.Sandbox,
  url: System.get_env("SANDBOX_URL") || "wss://sandbox.example.com/ws",
  auth: [
    api_key: System.get_env("SANDBOX_API_KEY"),
    refresh_url: System.get_env("SANDBOX_REFRESH_URL"),  # Optional
    refresh_interval_ms: 3_600_000  # 1 hour
  ],
  pool: [
    size: String.to_integer(System.get_env("SANDBOX_POOL_SIZE") || "5"),
    max_overflow: String.to_integer(System.get_env("SANDBOX_POOL_OVERFLOW") || "10")
  ],
  timeouts: [
    connect_timeout_ms: 5_000,
    execution_default_ms: 30_000,
    execution_max_ms: 300_000
  ],
  limits: [
    default_memory_mb: 256,
    max_memory_mb: 1024,
    default_cpu_shares: 512
  ]
```

**Configuration module:**
```elixir
defmodule AgentFramework.Sandbox.Config do
  @moduledoc """
  Runtime configuration for sandbox connections.
  """

  def url do
    get_in(config(), [:url]) || raise "SANDBOX_URL not configured"
  end

  def api_key do
    get_in(config(), [:auth, :api_key]) || raise "SANDBOX_API_KEY not configured"
  end

  def refresh_url do
    get_in(config(), [:auth, :refresh_url])
  end

  def refresh_interval_ms do
    get_in(config(), [:auth, :refresh_interval_ms]) || 3_600_000
  end

  def pool_size do
    get_in(config(), [:pool, :size]) || 5
  end

  def pool_max_overflow do
    get_in(config(), [:pool, :max_overflow]) || 10
  end

  def connect_timeout_ms do
    get_in(config(), [:timeouts, :connect_timeout_ms]) || 5_000
  end

  def default_memory_mb do
    get_in(config(), [:limits, :default_memory_mb]) || 256
  end

  defp config do
    Application.get_env(:agent_framework, AgentFramework.Sandbox, [])
  end
end
```

**Token refresh flow:**
```elixir
defmodule AgentFramework.Sandbox.TokenManager do
  use GenServer

  @moduledoc """
  Manages API key refresh for sandbox authentication.
  Optional - only active if refresh_url is configured.
  """

  def start_link(opts) do
    GenServer.start_link(__MODULE__, opts, name: __MODULE__)
  end

  def get_token do
    GenServer.call(__MODULE__, :get_token)
  end

  def init(_opts) do
    state = %{
      current_token: Config.api_key(),
      expires_at: nil
    }

    if Config.refresh_url() do
      schedule_refresh()
    end

    {:ok, state}
  end

  def handle_call(:get_token, _from, state) do
    {:reply, state.current_token, state}
  end

  def handle_info(:refresh, state) do
    case refresh_token(state.current_token) do
      {:ok, new_token, expires_at} ->
        schedule_refresh()
        {:noreply, %{state | current_token: new_token, expires_at: expires_at}}

      {:error, reason} ->
        Logger.warning("Token refresh failed", reason: reason)
        # Retry sooner on failure
        Process.send_after(self(), :refresh, 60_000)
        {:noreply, state}
    end
  end

  defp schedule_refresh do
    Process.send_after(self(), :refresh, Config.refresh_interval_ms())
  end

  defp refresh_token(current_token) do
    # POST to refresh URL with current token
    case Req.post(Config.refresh_url(), json: %{token: current_token}) do
      {:ok, %{status: 200, body: %{"token" => new_token, "expires_at" => exp}}} ->
        {:ok, new_token, exp}

      {:ok, %{status: status}} ->
        {:error, {:http_status, status}}

      {:error, reason} ->
        {:error, reason}
    end
  end
end
```

**WebSocket connection with auth:**
```elixir
defp connect_websocket(url, _auth) do
  token = TokenManager.get_token()

  headers = [
    {"Authorization", "Bearer #{token}"},
    {"X-Protocol-Version", "1"}
  ]

  case WebSockex.start_link(url, __MODULE__, %{}, extra_headers: headers) do
    {:ok, pid} -> {:ok, pid}
    {:error, reason} -> {:error, reason}
  end
end
```

**Environment variables summary:**

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `SANDBOX_URL` | Yes | - | WebSocket URL for sandbox |
| `SANDBOX_API_KEY` | Yes | - | Initial API key |
| `SANDBOX_REFRESH_URL` | No | - | Token refresh endpoint |
| `SANDBOX_POOL_SIZE` | No | 5 | Base connection pool size |
| `SANDBOX_POOL_OVERFLOW` | No | 10 | Max overflow connections |

**Rationale:**
- Runtime config allows deployment-time changes without rebuild
- API key + refresh token supports both static and rotating credentials
- TokenManager handles refresh automatically in background
- Pool configuration tunable per deployment environment

---

### Dimension 21: Complete

All gaps filled for Sandbox Interface:

| Gap | Decision |
|-----|----------|
| Connection lifecycle | Pooled connections with immediate + exponential backoff |
| Protocol errors | Fail with retryable/non-retryable ToolError based on error type |
| Tool executor routing | Check sandbox_mode() at execution time |
| Protocol specification | Fathom Sandbox Protocol v1.0 (JSON, versioned, streaming) |
| Configuration | Runtime env config; API key with optional refresh token |

---

## ToolError Gap Resolution

### ToolError Gap 1: Integration with Tool Executor Flow

**Decision:** All tool errors wrapped in ToolError; executor normalizes any raw errors

**Tool behaviour contract:**
```elixir
defmodule AgentFramework.Tool do
  @moduledoc """
  Behaviour for tool implementations.
  Tools MUST return {:ok, result} or {:error, %ToolError{}}.
  """

  alias AgentFramework.Schemas.ToolError

  @type result :: {:ok, term()} | {:error, ToolError.t()}

  @callback execute(struct(), context :: map()) :: result()
  @callback sandbox_mode() :: :none | :external
  @callback changeset(struct(), map()) :: Ecto.Changeset.t()

  defmacro __using__(_opts) do
    quote do
      @behaviour AgentFramework.Tool
      use Ecto.Schema
      import Ecto.Changeset

      alias AgentFramework.Schemas.ToolError

      def sandbox_mode, do: :none
      defoverridable sandbox_mode: 0
    end
  end
end
```

**Executor with error normalization:**
```elixir
defmodule AgentFramework.Tools.Executor do
  alias AgentFramework.Schemas.ToolError

  def execute(tool_module, params, context) do
    with {:ok, validated} <- validate_params(tool_module, params),
         {:ok, result} <- execute_and_normalize(tool_module, validated, context) do
      {:ok, result}
    end
  end

  defp validate_params(tool_module, params) do
    changeset = tool_module.changeset(struct(tool_module), params)

    if changeset.valid? do
      {:ok, Ecto.Changeset.apply_changes(changeset)}
    else
      {:error, ToolError.validation_error(
        tool_name(tool_module),
        format_changeset_errors(changeset),
        %{params: params, errors: changeset.errors}
      )}
    end
  end

  defp execute_and_normalize(tool_module, params, context) do
    result = case tool_module.sandbox_mode() do
      :none -> execute_direct(tool_module, params, context)
      :external -> execute_sandboxed(tool_module, params, context)
    end

    normalize_result(result, tool_module)
  end

  defp normalize_result({:ok, _} = success, _tool_module), do: success
  defp normalize_result({:error, %ToolError{}} = error, _tool_module), do: error

  # Catch raw errors and wrap them
  defp normalize_result({:error, reason}, tool_module) when is_binary(reason) do
    {:error, ToolError.execution_error(tool_name(tool_module), reason)}
  end

  defp normalize_result({:error, reason}, tool_module) do
    {:error, ToolError.execution_error(tool_name(tool_module), inspect(reason))}
  end

  # Catch exceptions from poorly-behaved tools
  defp execute_direct(tool_module, params, context) do
    task = Task.Supervisor.async_nolink(ToolTaskSupervisor, fn ->
      tool_module.execute(params, context)
    end)

    timeout = Map.get(context, :timeout_ms, 30_000)

    case Task.yield(task, timeout) || Task.shutdown(task) do
      {:ok, result} ->
        result

      nil ->
        {:error, ToolError.timeout_error(tool_name(tool_module), timeout)}

      {:exit, reason} ->
        {:error, ToolError.execution_error(
          tool_name(tool_module),
          "Tool crashed: #{inspect(reason)}",
          retryable: false
        )}
    end
  end
end
```

**Rationale:**
- Consistent error type simplifies downstream handling
- Tools that return raw errors get wrapped automatically
- Crashes caught and converted to ToolError
- Validation errors include original params for debugging

---

### ToolError Gap 2: format_for_llm Location and Invocation

**Decision:** Formatting defined in ToolError module; invoked by StepExecutor when building tool result message

**ToolError formatting:**
```elixir
defmodule AgentFramework.Schemas.ToolError do
  # ... existing schema definition ...

  @doc """
  Format error for LLM context.
  Returns a string suitable for inclusion in the conversation as a tool result.
  """
  def format_for_llm(%__MODULE__{} = error) do
    parts = [
      "Tool `#{error.tool_name}` failed.",
      "Error type: #{error.error_type}",
      "Message: #{error.message}"
    ]

    parts = if error.retryable do
      parts ++ ["This error may be resolved by trying again with different parameters."]
    else
      parts ++ ["This error is not retryable."]
    end

    parts = if map_size(error.context) > 0 && show_context?(error) do
      parts ++ ["Context: #{format_context(error.context)}"]
    else
      parts
    end

    Enum.join(parts, "\n")
  end

  # Only show context for certain error types (avoid exposing internals)
  defp show_context?(%{error_type: :validation}), do: true
  defp show_context?(%{error_type: :execution}), do: true
  defp show_context?(_), do: false

  defp format_context(context) do
    context
    |> Enum.map(fn {k, v} -> "#{k}: #{inspect(v)}" end)
    |> Enum.join(", ")
  end
end
```

**Invocation in StepExecutor:**
```elixir
defmodule AgentFramework.Session.StepExecutor do
  alias AgentFramework.Schemas.ToolError

  defp handle_response(session_id, step, %{tool_calls: calls}) when calls != [] do
    results = execute_tools(calls)

    # Format results for next LLM call
    formatted_results = Enum.map(results, fn
      {tool_call_id, {:ok, result}} ->
        %{
          tool_call_id: tool_call_id,
          role: "tool",
          content: format_tool_result(result)
        }

      {tool_call_id, {:error, %ToolError{} = error}} ->
        %{
          tool_call_id: tool_call_id,
          role: "tool",
          content: ToolError.format_for_llm(error)
        }
    end)

    # Store results (triggers telemetry)
    store_tool_results(session_id, results)

    # Continue LLM loop with tool results
    execute_llm_step(session_id, step, tool_results: formatted_results)
  end

  defp format_tool_result(result) when is_binary(result), do: result
  defp format_tool_result(result) when is_map(result), do: Jason.encode!(result)
  defp format_tool_result(result), do: inspect(result)
end
```

**Example LLM sees:**
```
Tool `web_search` failed.
Error type: timeout
Message: Execution timed out after 30000ms
This error is not retryable.
```

```
Tool `write_file` failed.
Error type: validation
Message: path is required; content must be a string
This error may be resolved by trying again with different parameters.
Context: params: %{path: nil, content: 123}
```

**Rationale:**
- ToolError owns formatting logic (single source of truth)
- StepExecutor invokes formatting when building tool result messages
- LLM receives clear, actionable error information
- Context shown selectively to avoid leaking internals

---

### ToolError Gap 3: Migration from Raw Errors

**Decision:** Executor auto-wraps raw errors; tools encouraged to use ToolError directly

**Migration strategy:**

1. **Executor handles legacy returns** (already implemented in Gap 1)
   - `{:error, reason}` wrapped into `ToolError.execution_error/2`
   - Tools continue to work without changes

2. **Encourage ToolError usage for new tools**
   - `use AgentFramework.Tool` imports ToolError helpers
   - Documentation shows ToolError as the standard pattern

3. **Built-in tools updated to use ToolError**
   - Return specific error types for better LLM guidance

**Before (legacy):**
```elixir
defmodule MyTools.WebSearch do
  def execute(%__MODULE__{query: query}, _context) do
    case Serper.search(query) do
      {:ok, results} -> {:ok, results}
      {:error, :rate_limited} -> {:error, "Rate limited, try again later"}
      {:error, :timeout} -> {:error, "Search timed out"}
      {:error, reason} -> {:error, inspect(reason)}
    end
  end
end
```

**After (with ToolError):**
```elixir
defmodule MyTools.WebSearch do
  use AgentFramework.Tool

  def execute(%__MODULE__{query: query}, _context) do
    case Serper.search(query) do
      {:ok, results} ->
        {:ok, results}

      {:error, :rate_limited} ->
        {:error, ToolError.execution_error("web_search",
          "Rate limited by search provider",
          retryable: true,
          context: %{retry_after_ms: 60_000}
        )}

      {:error, :timeout} ->
        {:error, ToolError.timeout_error("web_search", 30_000)}

      {:error, reason} ->
        {:error, ToolError.execution_error("web_search", inspect(reason))}
    end
  end
end
```

**Helper for common patterns:**
```elixir
defmodule AgentFramework.Schemas.ToolError do
  # Convenience for wrapping external API errors
  def from_api_error(tool_name, %{status: status, body: body}) when status >= 400 do
    retryable = status in [429, 500, 502, 503, 504]

    %__MODULE__{
      tool_name: tool_name,
      error_type: :execution,
      message: "API error: #{status}",
      retryable: retryable,
      context: %{status: status, body: body}
    }
  end

  def from_exception(tool_name, %{__exception__: true} = exception) do
    %__MODULE__{
      tool_name: tool_name,
      error_type: :execution,
      message: Exception.message(exception),
      retryable: false,
      context: %{exception_type: exception.__struct__}
    }
  end
end
```

**Rationale:**
- Backward compatible: legacy tools continue working
- Progressive adoption: tools can migrate incrementally
- Better LLM guidance: specific error types enable smarter retry decisions
- Helpers reduce boilerplate for common error patterns

---

### ToolError: Complete

All gaps filled for ToolError refinement:

| Gap | Decision |
|-----|----------|
| Executor integration | All errors wrapped in ToolError; executor normalizes raw errors |
| format_for_llm location | Defined in ToolError module; called by StepExecutor |
| Migration path | Auto-wrap for backward compat; encourage ToolError for new tools |

---

## Session Summary

**Date**: 2025-12-27 (Session 2)
**Duration**: Gap resolution for Dimensions 20-21 and ToolError

### Decisions Made

#### Dimension 20: Hierarchical Memory (5 gaps filled)

| Gap | Decision |
|-----|----------|
| Summary trigger | Token threshold (4000 unsummarized tokens) |
| Summary mode | Full replacement (not incremental) |
| Cache invalidation | No TTL; token-triggered regeneration only |
| Semantic retrieval | Per-message embedding, user message query, top-K with token accumulation |
| Output format | Structured `%Context{}` struct with semantic attribution markers |
| Integration point | Called once per LLM call in StepExecutor |

#### Dimension 21: Sandbox Interface (5 gaps filled)

| Gap | Decision |
|-----|----------|
| Connection lifecycle | Pooled (Poolboy), immediate + exponential backoff reconnection |
| Protocol errors | Retryable errors on disconnect; log + fail on malformed responses |
| Routing | Check `sandbox_mode()` at execution time |
| Protocol | **Fathom Sandbox Protocol v1.0** - JSON, versioned, streaming, stateless |
| Configuration | Runtime env config; API key with optional refresh token |

#### ToolError Refinement (3 gaps filled)

| Gap | Decision |
|-----|----------|
| Executor integration | All errors wrapped in ToolError; auto-normalization |
| format_for_llm | Defined in ToolError module; invoked by StepExecutor |
| Migration | Backward compatible; progressive adoption encouraged |

### New Artifacts

1. **Fathom Sandbox Protocol v1.0** - Complete application-level protocol specification
   - Message types: execute, cancel, ping, ack, status, stdout, stderr, result, error, pong
   - Error codes: TIMEOUT, OOM, OUTPUT_LIMIT, LANGUAGE_NOT_SUPPORTED, etc.
   - Sequence diagrams for success, timeout, cancel, and error cases

2. **Context struct** - New schema for assembled LLM context with tier provenance

3. **Accumulator module** - Client-side event accumulation for sandbox protocol

### Next Steps

1. ~~Update `docs/elixir-design.md` with gap resolutions~~ Done
2. ~~Extract FSP v1.0 to standalone protocol document~~ Done → `docs/fathom-sandbox-protocol.md`
3. Proceed to implementation planning or identify remaining gaps

---
